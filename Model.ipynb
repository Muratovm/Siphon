{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Reservoir.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from Reservoir import Reservoir\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training stacks 5400\n",
      "Total validation stacks 600\n",
      "Total testing stacks 10000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = Reservoir(\"./data/training/images/train-images-idx3-ubyte.gz\",\n",
    "                        \"./data/training/labels/train-labels-idx1-ubyte.gz\",\n",
    "                        \"./data/testing/images/t10k-images-idx3-ubyte.gz\",\n",
    "                        \"./data/testing/labels/t10k-labels-idx1-ubyte.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n",
      "units after conv 588\n",
      "fc parameters:  589\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,outputs,image_shape):\n",
    "        super(ConvNet, self).__init__()\n",
    "        img_size = list(image_shape)\n",
    "        img_size = torch.Size([1] + img_size)\n",
    "        empty = torch.zeros(img_size)\n",
    "        \n",
    "        channels = 3\n",
    "        kernel = 3\n",
    "        padding = 1\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(image_shape[0],\n",
    "                                             out_channels = channels,\n",
    "                                             kernel_size = kernel,\n",
    "                                             padding = padding),\n",
    "                                  nn.BatchNorm2d(channels),\n",
    "                                  nn.MaxPool2d(2),\n",
    "                                  nn.ReLU())\n",
    "        units = self.conv1(empty).numel()\n",
    "        print(\"units after conv\", units)\n",
    "        self.fc = nn.Sequential(nn.Linear(units, outputs))\n",
    "        print(\"fc parameters: \",sum(p.numel() for p in self.fc.parameters()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #x: batch, channel, height, width\n",
    "        batch_size = len(x)\n",
    "        out = self.conv1(x)\n",
    "        out = out.reshape((batch_size,-1))\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "    def load_weights(self,path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.start_epoch = checkpoint['epoch']\n",
    "        \n",
    "    def save_weights(self,optimizer,epoch,path):\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()}, \n",
    "            path)\n",
    "       \n",
    "    def current_snapshot_name():\n",
    "        from time import gmtime, strftime\n",
    "        import socket\n",
    "\n",
    "        hostname = socket.gethostname()\n",
    "\n",
    "        date = strftime(\"%b%d_\", gmtime())\n",
    "        clock = strftime(\"%X\", gmtime())\n",
    "        now = clock.split(\":\")\n",
    "        now = date+'-'.join(now)\n",
    "\n",
    "        name = now+\"_\"+hostname\n",
    "        return name\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    image = dataset.training_pool[0]['image']\n",
    "    image = image.reshape((1,28,28))\n",
    "    net = ConvNet(1,image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "input torch.Size([10, 1, 28, 28])\n",
      "output tensor([[-0.8686],\n",
      "        [-1.2079],\n",
      "        [-1.0248],\n",
      "        [-1.2757],\n",
      "        [-1.0928],\n",
      "        [-0.3885],\n",
      "        [-0.8365],\n",
      "        [-0.8118],\n",
      "        [-1.5839],\n",
      "        [-1.0070]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataloader = dataset.training_pool.dataloader\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i > 0:\n",
    "            break\n",
    "        \n",
    "        imgs = batch['image'].float()\n",
    "        imgs = np.transpose(imgs,(0,3,1,2))\n",
    "        print(imgs[0].shape)\n",
    "        print(\"input\", imgs.shape)\n",
    "        out = net(imgs)\n",
    "        print(\"output\", out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
